{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd1ca570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "473d2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('/Users/rajesh/Documents/Datasets/names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35fcba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a44ac0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "  \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2874f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3d7ad5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a10aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dcf3a9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3656, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "977d4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 10])\n",
      "torch.Size([27, 10])\n",
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "print(emb.shape)\n",
    "print(C.shape)\n",
    "print(Xb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a46d4e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Run the backward pass manually, calculating the gradients manually\n",
    "# Compare our answer with the PyTorch autograd calculation\n",
    "\n",
    "# ------\n",
    "#logprobs.shape = torch.Size([32, 27])\n",
    "#dlogprobs should have the same shape\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "# ------\n",
    "\n",
    "# ------\n",
    "# d/dx(ln x) = 1/x\n",
    "dprobs = 1.0/probs * dlogprobs\n",
    "\n",
    "cmp('probs', dprobs, probs)\n",
    "# ------\n",
    "\n",
    "# ------\n",
    "# counts_sum.shape = torch.Size([32, 27])\n",
    "# counts_sum_inv.shape = torch.Size([32, 1])\n",
    "# Pytorch does a broadcast of counts_sum_inv replicating the column vector 27 times to facilitate an element-wise multiplication\n",
    "\n",
    "# Simple example:\n",
    "# c = a * b (with tensors)\n",
    "# c (3x3), a (3x3), b(3x1)\n",
    "# c = \n",
    "# a11*b1     a12*b1     a13*b1\n",
    "# a21*b2     a22*b2     a23*b2\n",
    "# a31*b3     a32*b3     a33*b3\n",
    "\n",
    "# dc/da11 = b1\n",
    "# dc/db1 = (a11 + a12 + a13) -- add up the component gradients\n",
    "\n",
    "# a in the above example is counts\n",
    "# b in the ablve example is counts_sum_inv\n",
    "# Note that counts_sum_inv was a broadcast operation. We need to add up the different component gradients.\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "# ------\n",
    "\n",
    "# ------\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "\n",
    "# Note that counts_sum_inv dependes on counts_sum which in turn depends on counts\n",
    "# counts_sum = counts ** -1\n",
    "# dcounts_sum = -1 counts ** -2 (remember d/dx of x^n = n * x^(n-1))\n",
    "dcounts_sum = (-counts_sum ** -2.0) * dcounts_sum_inv\n",
    "\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "# ------\n",
    "\n",
    "# ------\n",
    "# Note the + below is to ensure that we are adding the different gradient components together\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "cmp('counts', dcounts, counts)\n",
    "# ------\n",
    "\n",
    "# ------\n",
    "# Note d/dx (e^x) = e^x\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "# ------\n",
    "\n",
    "# ------\n",
    "# norm_logits = logits - logit_maxes\n",
    "# Think of this as c = a - b\n",
    "# c (3 x 3), a (3 x 3), b (3 x 1) -- broadcast\n",
    "\n",
    "dlogits = torch.ones_like(norm_logits) * dnorm_logits\n",
    "dlogit_maxes = (- torch.ones_like(norm_logits) * dnorm_logits).sum(1, keepdim=True)\n",
    "\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "\n",
    "# Keep in mind that dlogit_maxes = 0 because subtracting the max does not change the gradient.\n",
    "# i.e., gradient does not depend on logit_maxes\n",
    "# In practice you get very small gradients i.e., 1.0 e-9\n",
    "\n",
    "# ------\n",
    "# logit_maxes is picking up the max from each row in logits\n",
    "#dlogits_temp = torch.zeros_like(logits)\n",
    "#dlogits_temp[range(n),logits.max(1,keepdim=True).indices] = dlogit_maxes\n",
    "#dlogits += dlogits_temp\n",
    "\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "cmp('logits', dlogits, logits)\n",
    "# ------\n",
    "\n",
    "# ------\n",
    "\n",
    "# d = a @ b + c\n",
    "# |d11   d12| = |a11   a12| *  |b11   b12| +  |c1   c2|\n",
    "# |d21   d22| = |a21   a22|    |b21   b22|    |c1   c2|\n",
    "# d11 = a11*b11 + a12*b21 + c1\n",
    "# d12 = a11*b12 + a12*b22 + c2\n",
    "# d21 = a21*b11 + a22*b21 + c1\n",
    "# d22 = a21*b12 + a22*b22 + c2\n",
    "\n",
    "# If we write out the derivatives:\n",
    "# dL/da = dL/dd * B' (B transpose)\n",
    "# Backward pass of the matrix multiply is a matrix multiply\n",
    "\n",
    "dh = dlogits @ W2.T\n",
    "\n",
    "cmp('h', dh, h)\n",
    "\n",
    "# -----\n",
    "dW2 = h.T @dlogits\n",
    "\n",
    "cmp('W2', dW2, W2)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "# dlogits is (32 x 27)\n",
    "# b2 is (27)\n",
    "# db2 = dlogits.sum(0)\n",
    "db2 = dlogits.sum(0)\n",
    "cmp('b2', db2, b2)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "# Derivative of tanh = 1 - tanh**2\n",
    "dhpreact = (1 - h**2) * dh\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "# -----\n",
    "\n",
    "\n",
    "# -----\n",
    "# print(hpreact.shape) torch.Size([32, 64])\n",
    "# print(bngain.shape) torch.Size([1, 64])\n",
    "# print(bnraw.shape)  torch.Size([32, 64])\n",
    "# print(bnbias.shape) torch.Size([1, 64])\n",
    "\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "\n",
    "cmp('bngain', dbngain, bngain)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "dbnraw = bngain*dhpreact\n",
    "\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "# bnraw = bndiff * bnvar_inv (element wise multiplication)\n",
    "dbndiff = dbnraw * bnvar_inv\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim = True)\n",
    "\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "dbndiff2 = 1.0/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "dbndiff += 2 * (bndiff ** 1) * dbndiff2\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "# BatchNorm layer\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "\n",
    "dbnmeani = (-1 * dbndiff).sum(0, keepdim=True)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "#print(bndiff.shape)\n",
    "#print(hprebn.shape)\n",
    "#print(bnmeani.shape)\n",
    "\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += 1.0/n * torch.ones_like(hprebn) * dbnmeani\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "# Linear layer 1\n",
    "# hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "#print(hprebn.shape) - torch.Size([32, 64])\n",
    "#print(embcat.shape) - torch.Size([32, 30])\n",
    "#print(W1.shape) - torch.Size([30, 64])\n",
    "#print(b1.shape) - torch.Size([64])\n",
    "\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "\n",
    "# -----\n",
    "\n",
    "# -----\n",
    "# emb = C[Xb] # embed the characters into vectors\n",
    "# embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "demb = dembcat.view(emb.shape)\n",
    "cmp('emb', demb, emb)\n",
    "\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        idx = Xb[i,j]\n",
    "        dC[idx] += demb[i,j]\n",
    "\n",
    "cmp('C', dC, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27f9be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.365561008453369 diff: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b24a7a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.2177499532699585e-09\n"
     ]
    }
   ],
   "source": [
    "dlogits = F.softmax(logits, 1)\n",
    "\n",
    "# Subtract 1 from the correct probability\n",
    "dlogits[range(n), Yb] -= 1\n",
    "\n",
    "# Backpropagate through the average loss\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a66f7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0707, 0.0881, 0.0178, 0.0486, 0.0215, 0.0819, 0.0251, 0.0366, 0.0181,\n",
       "        0.0328, 0.0350, 0.0373, 0.0326, 0.0290, 0.0367, 0.0143, 0.0105, 0.0189,\n",
       "        0.0159, 0.0521, 0.0510, 0.0205, 0.0251, 0.0746, 0.0603, 0.0237, 0.0211],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st example\n",
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c62b3011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0707,  0.0881,  0.0178,  0.0486,  0.0215,  0.0819,  0.0251,  0.0366,\n",
       "        -0.9819,  0.0328,  0.0350,  0.0373,  0.0326,  0.0290,  0.0367,  0.0143,\n",
       "         0.0105,  0.0189,  0.0159,  0.0521,  0.0510,  0.0205,  0.0251,  0.0746,\n",
       "         0.0603,  0.0237,  0.0211], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at dlogits for the same example\n",
    "# scale up by n\n",
    "\n",
    "# Identical to the F.softmax above except the correct example which is softmax - 1\n",
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f2ae9a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.8626e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of dlogits == 0\n",
    "\n",
    "# Intuitively - the gradients get distributed across all the classes\n",
    "# The correct class is boosted while the incorrect classes are pulled down\n",
    "\n",
    "# The amount of push and pull is proportional to the probabilities that were computed in the forward pass\n",
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16fd8d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdba0ee8790>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intuitive understanding of dlogits\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "447f17a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a single formula for backpropagating through the batch normalization layer\n",
    "\n",
    "# Calculate dhprebn given dhpreact using an analytical formula\n",
    "\n",
    "\n",
    "dhprebn = bngain * bnvar_inv/n * (n * dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact * bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
